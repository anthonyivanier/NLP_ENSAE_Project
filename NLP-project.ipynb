{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chargement des données des prénoms\n",
    "df_prenoms = pd.read_csv('/Users/anthonyivanier/Desktop/Ensae/3A/NLP/firstname_with_sex.csv', sep=';')\n",
    "\n",
    "# Chargement des transcriptions\n",
    "df_transcriptions = pd.read_csv('/Users/anthonyivanier/Desktop/Ensae/3A/NLP/transcriptions_with_sex.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firstname</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marie</td>\n",
       "      <td>10145</td>\n",
       "      <td>2390322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jean</td>\n",
       "      <td>1869615</td>\n",
       "      <td>6476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pierre</td>\n",
       "      <td>1475841</td>\n",
       "      <td>5047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jeanne</td>\n",
       "      <td>1765</td>\n",
       "      <td>1097397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>françois</td>\n",
       "      <td>1089009</td>\n",
       "      <td>5951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>joseph</td>\n",
       "      <td>897742</td>\n",
       "      <td>4246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>anne</td>\n",
       "      <td>1479</td>\n",
       "      <td>816241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>marguerite</td>\n",
       "      <td>1441</td>\n",
       "      <td>813859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>catherine</td>\n",
       "      <td>1223</td>\n",
       "      <td>792448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>louis</td>\n",
       "      <td>750498</td>\n",
       "      <td>2720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>françoise</td>\n",
       "      <td>1153</td>\n",
       "      <td>600167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jacques</td>\n",
       "      <td>585567</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>antoine</td>\n",
       "      <td>536089</td>\n",
       "      <td>2067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nicolas</td>\n",
       "      <td>408007</td>\n",
       "      <td>1463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>louise</td>\n",
       "      <td>681</td>\n",
       "      <td>360914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>claude</td>\n",
       "      <td>324134</td>\n",
       "      <td>4830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>charles</td>\n",
       "      <td>276133</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>elisabeth</td>\n",
       "      <td>353</td>\n",
       "      <td>224969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>madeleine</td>\n",
       "      <td>424</td>\n",
       "      <td>217939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>michel</td>\n",
       "      <td>214487</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>etienne</td>\n",
       "      <td>211297</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>guillaume</td>\n",
       "      <td>197634</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>claudine</td>\n",
       "      <td>280</td>\n",
       "      <td>163711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>andré</td>\n",
       "      <td>160970</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>yves</td>\n",
       "      <td>156916</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>henri</td>\n",
       "      <td>151366</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>antoinette</td>\n",
       "      <td>326</td>\n",
       "      <td>150155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     firstname     male   female\n",
       "0        marie    10145  2390322\n",
       "1         jean  1869615     6476\n",
       "2       pierre  1475841     5047\n",
       "3       jeanne     1765  1097397\n",
       "4     françois  1089009     5951\n",
       "5       joseph   897742     4246\n",
       "6         anne     1479   816241\n",
       "7   marguerite     1441   813859\n",
       "8    catherine     1223   792448\n",
       "9        louis   750498     2720\n",
       "10   françoise     1153   600167\n",
       "11     jacques   585567     2010\n",
       "12     antoine   536089     2067\n",
       "13     nicolas   408007     1463\n",
       "14      louise      681   360914\n",
       "15      claude   324134     4830\n",
       "16     charles   276133      972\n",
       "17   elisabeth      353   224969\n",
       "18   madeleine      424   217939\n",
       "19      michel   214487      834\n",
       "20     etienne   211297      898\n",
       "21   guillaume   197634      792\n",
       "22    claudine      280   163711\n",
       "23       andré   160970      586\n",
       "24        yves   156916      544\n",
       "25       henri   151366      504\n",
       "26  antoinette      326   150155"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prenoms.head(27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: firstname, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "df_prenoms_with_space_or_hyphen = df_prenoms[df_prenoms['firstname'].str.contains(' |-', regex=True)]\n",
    "print(df_prenoms_with_space_or_hyphen['firstname'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6946, 3) (241, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_prenoms.shape , df_transcriptions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_line</th>\n",
       "      <th>groundtruth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ebb26ada-044c-4c62-9dbc-a9c8d505d31c</td>\n",
       "      <td>surname: Chardon firstname: Marie occupation: ...</td>\n",
       "      <td>nom: Chardon prénom: Marie date_naissance: 30 ...</td>\n",
       "      <td>femme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>338496f5-e4ca-43ac-aa5c-429cb3f6ac00</td>\n",
       "      <td>surname: Lhopital firstname: Louis-Jean occupa...</td>\n",
       "      <td>nom: Lhopital prénom: Louis Jean date_naissanc...</td>\n",
       "      <td>homme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e6a279da-9b6f-4f49-b498-64857bc50d1e</td>\n",
       "      <td>surname: Papin firstname: Marie occupation: id...</td>\n",
       "      <td>nom: Pyrin prénom: Marie date_naissance: 55 re...</td>\n",
       "      <td>femme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7534deca-39e8-4f00-be17-c12460015de1</td>\n",
       "      <td>surname: Lavocat firstname: Marie link: femme ...</td>\n",
       "      <td>nom: Lavocat prénom: Marie date_naissance: 187...</td>\n",
       "      <td>femme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ef334a66-a504-418a-9872-e7c9db923488</td>\n",
       "      <td>surname: Benne firstname: Marguerite age: 78</td>\n",
       "      <td>nom: Benne prénom: Marguerite date_naissance: ...</td>\n",
       "      <td>femme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           subject_line  \\\n",
       "0  ebb26ada-044c-4c62-9dbc-a9c8d505d31c   \n",
       "1  338496f5-e4ca-43ac-aa5c-429cb3f6ac00   \n",
       "2  e6a279da-9b6f-4f49-b498-64857bc50d1e   \n",
       "3  7534deca-39e8-4f00-be17-c12460015de1   \n",
       "4  ef334a66-a504-418a-9872-e7c9db923488   \n",
       "\n",
       "                                         groundtruth  \\\n",
       "0  surname: Chardon firstname: Marie occupation: ...   \n",
       "1  surname: Lhopital firstname: Louis-Jean occupa...   \n",
       "2  surname: Papin firstname: Marie occupation: id...   \n",
       "3  surname: Lavocat firstname: Marie link: femme ...   \n",
       "4      surname: Benne firstname: Marguerite age: 78    \n",
       "\n",
       "                                          prediction    sex  \n",
       "0  nom: Chardon prénom: Marie date_naissance: 30 ...  femme  \n",
       "1  nom: Lhopital prénom: Louis Jean date_naissanc...  homme  \n",
       "2  nom: Pyrin prénom: Marie date_naissance: 55 re...  femme  \n",
       "3  nom: Lavocat prénom: Marie date_naissance: 187...  femme  \n",
       "4  nom: Benne prénom: Marguerite date_naissance: ...  femme  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transcriptions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_line                                       ebb26ada-044c-4c62-9dbc-a9c8d505d31c\n",
       "groundtruth     surname: Chardon firstname: Marie occupation: idem link: fille age: 30 \n",
       "prediction            nom: Chardon prénom: Marie date_naissance: 30 lieux_naissance: \" \n",
       "sex                                                                               femme\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Set the display option to print the entire line\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_transcriptions.iloc[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probalité des sexes des prénoms\n",
    "df_prenoms['total'] = df_prenoms['male'] + df_prenoms['female']\n",
    "df_prenoms['gender_probability'] = df_prenoms['female'] / df_prenoms['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's exctract first_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def extract_firstname(text):\n",
    "    # Extraction du prénom à partir de la colonne groundtruth ou prediction\n",
    "    parts = text.split()\n",
    "    for i, part in enumerate(parts):\n",
    "        if part in [\"firstname:\", \"prénom:\"]:\n",
    "            return parts[i+1]  # Retourne le prénom qui suit ces indicateurs\n",
    "    return None\n",
    "    '''\n",
    "\n",
    "def extract_firstname(text):\n",
    "    # Extraction du prénom à partir de la colonne groundtruth ou prediction\n",
    "    parts = text.split()\n",
    "    for i, part in enumerate(parts):\n",
    "        if part in [\"firstname:\", \"prénom:\"]:\n",
    "            firstname = parts[i+1]  \n",
    "            \n",
    "            \n",
    "            if i+2 < len(parts) and parts[i+2].isalpha():\n",
    "                firstname += \" \" + parts[i+2]\n",
    "            \n",
    "            return firstname\n",
    "    return None\n",
    "\n",
    "\n",
    "df_transcriptions['extracted_firstname_gt'] = df_transcriptions['groundtruth'].apply(extract_firstname)\n",
    "df_transcriptions['extracted_firstname_gt'] = df_transcriptions['extracted_firstname_gt'].str.lower()\n",
    "\n",
    "#\n",
    "df_transcriptions['extracted_firstname_pred'] = df_transcriptions['prediction'].apply(extract_firstname)  \n",
    "df_transcriptions['extracted_firstname_pred'] = df_transcriptions['extracted_firstname_pred'].str.lower()\n",
    "\n",
    "# Column 'sex' en binaire\n",
    "df_transcriptions['sex_binary'] = df_transcriptions['sex'].map({'femme': 1, 'homme': 0, 'ambigu': float('nan')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "louis-jean louis jean\n",
      "antoine angène\n",
      "alexandre herandre\n",
      "marguerite oarguerite\n",
      "virginie vigmie\n",
      "eugènie eugénie\n",
      "josèphine joséphine\n",
      "anroine antoine\n",
      "justine gustine\n",
      "clément clement\n",
      "antoine angloise\n",
      "françois feris\n",
      "eugènie eugénie\n",
      "jacques jregues\n",
      "claude vaude\n",
      "jean-marie jean marie\n",
      "gilbert gilbeuse\n",
      "branislav branistau\n",
      "stanislas mamiolas\n",
      "None eugéne\n",
      "madeleine gadeleine\n",
      "etiennette ctiennette\n",
      "antoine aupène\n",
      "claude clause\n",
      "gilbert gilbeup\n",
      "pierre anne\n",
      "marthe marthy\n",
      "alix alice\n",
      "françoise faul\n",
      "françois antoine zean antoine\n",
      "antonie entonie\n",
      "jean-baptiste jean-haptiste\n",
      "jean-claude jean blanse\n",
      "victor vicher\n",
      "madeleine mareleine\n",
      "jean raymond jean-raymond\n",
      "france franco\n",
      "blaise clause\n",
      "baptiste bt\n",
      "simon simone\n",
      "marie-louise marie s\n",
      "madeleine vadeleine\n",
      "théodore théodote\n",
      "barthélémy barthe\n",
      "françois ferd\n",
      "pétronille gihromille\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(df_transcriptions)):\n",
    "    if df_transcriptions['extracted_firstname_gt'].iloc[i] != df_transcriptions['extracted_firstname_pred'].iloc[i]:\n",
    "        print(df_transcriptions['extracted_firstname_gt'].iloc[i], df_transcriptions['extracted_firstname_pred'].iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_line</th>\n",
       "      <th>groundtruth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>sex</th>\n",
       "      <th>extracted_firstname_gt</th>\n",
       "      <th>extracted_firstname_pred</th>\n",
       "      <th>sex_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ebb26ada-044c-4c62-9dbc-a9c8d505d31c</td>\n",
       "      <td>surname: Chardon firstname: Marie occupation: idem link: fille age: 30</td>\n",
       "      <td>nom: Chardon prénom: Marie date_naissance: 30 lieux_naissance: \"</td>\n",
       "      <td>femme</td>\n",
       "      <td>marie</td>\n",
       "      <td>marie</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>338496f5-e4ca-43ac-aa5c-429cb3f6ac00</td>\n",
       "      <td>surname: Lhopital firstname: Louis-Jean occupation: sp link: chef age: 67</td>\n",
       "      <td>nom: Lhopital prénom: Louis Jean date_naissance: 67 lieux_naissance: Sn employeur: ahef</td>\n",
       "      <td>homme</td>\n",
       "      <td>louis-jean</td>\n",
       "      <td>louis jean</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e6a279da-9b6f-4f49-b498-64857bc50d1e</td>\n",
       "      <td>surname: Papin firstname: Marie occupation: idem link: idem employer: idem age: 15</td>\n",
       "      <td>nom: Pyrin prénom: Marie date_naissance: 55 relation: d</td>\n",
       "      <td>femme</td>\n",
       "      <td>marie</td>\n",
       "      <td>marie</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           subject_line  \\\n",
       "0  ebb26ada-044c-4c62-9dbc-a9c8d505d31c   \n",
       "1  338496f5-e4ca-43ac-aa5c-429cb3f6ac00   \n",
       "2  e6a279da-9b6f-4f49-b498-64857bc50d1e   \n",
       "\n",
       "                                                                           groundtruth  \\\n",
       "0              surname: Chardon firstname: Marie occupation: idem link: fille age: 30    \n",
       "1           surname: Lhopital firstname: Louis-Jean occupation: sp link: chef age: 67    \n",
       "2  surname: Papin firstname: Marie occupation: idem link: idem employer: idem age: 15    \n",
       "\n",
       "                                                                                 prediction  \\\n",
       "0                         nom: Chardon prénom: Marie date_naissance: 30 lieux_naissance: \"    \n",
       "1  nom: Lhopital prénom: Louis Jean date_naissance: 67 lieux_naissance: Sn employeur: ahef    \n",
       "2                                  nom: Pyrin prénom: Marie date_naissance: 55 relation: d    \n",
       "\n",
       "     sex extracted_firstname_gt extracted_firstname_pred  sex_binary  \n",
       "0  femme                  marie                    marie         1.0  \n",
       "1  homme             louis-jean               louis jean         0.0  \n",
       "2  femme                  marie                    marie         1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transcriptions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's exctract other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_occupation(text):\n",
    "    parts = text.split()\n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"occupation:\":\n",
    "            return parts[i+1]\n",
    "    return None\n",
    "\n",
    "def extract_link(text):\n",
    "    parts = text.split()\n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"link:\":\n",
    "            return parts[i+1]\n",
    "    return None\n",
    "\n",
    "def extract_profession(text):\n",
    "    parts = text.split()\n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"profession:\":\n",
    "            return parts[i+1]\n",
    "    return None\n",
    "\n",
    "def extract_relation(text):\n",
    "    parts = text.split()\n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"relation:\":\n",
    "            return parts[i+1]\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge based on extracted firstname\n",
    "df_combined = pd.merge(df_transcriptions,\n",
    "                       df_prenoms[['firstname', 'gender_probability']],\n",
    "                       left_on='extracted_firstname_gt',\n",
    "                       right_on='firstname',\n",
    "                       how='left')\n",
    "\n",
    "\n",
    "df_combined_pred = pd.merge(df_transcriptions,\n",
    "                       df_prenoms[['firstname', 'gender_probability']],\n",
    "                       left_on='extracted_firstname_pred',  # using predicted instead of groundtruth\n",
    "                       right_on='firstname',\n",
    "                       how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcriptions['occupation'] = df_combined['groundtruth'].apply(extract_occupation)\n",
    "df_transcriptions['link'] = df_combined['groundtruth'].apply(extract_link)\n",
    "df_transcriptions['profession'] = df_combined_pred['prediction'].apply(extract_profession)\n",
    "df_transcriptions['relation'] = df_combined_pred['prediction'].apply(extract_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['occupation'] = df_combined['groundtruth'].apply(extract_occupation)\n",
    "df_combined['link'] = df_combined['groundtruth'].apply(extract_link)\n",
    "df_combined_pred['profession'] = df_combined_pred['prediction'].apply(extract_profession)\n",
    "df_combined_pred['relation'] = df_combined_pred['prediction'].apply(extract_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_line</th>\n",
       "      <th>groundtruth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>sex</th>\n",
       "      <th>extracted_firstname_gt</th>\n",
       "      <th>extracted_firstname_pred</th>\n",
       "      <th>sex_binary</th>\n",
       "      <th>firstname</th>\n",
       "      <th>gender_probability</th>\n",
       "      <th>occupation</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ebb26ada-044c-4c62-9dbc-a9c8d505d31c</td>\n",
       "      <td>surname: Chardon firstname: Marie occupation: idem link: fille age: 30</td>\n",
       "      <td>nom: Chardon prénom: Marie date_naissance: 30 lieux_naissance: \"</td>\n",
       "      <td>femme</td>\n",
       "      <td>marie</td>\n",
       "      <td>marie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>marie</td>\n",
       "      <td>0.995774</td>\n",
       "      <td>idem</td>\n",
       "      <td>fille</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>338496f5-e4ca-43ac-aa5c-429cb3f6ac00</td>\n",
       "      <td>surname: Lhopital firstname: Louis-Jean occupation: sp link: chef age: 67</td>\n",
       "      <td>nom: Lhopital prénom: Louis Jean date_naissance: 67 lieux_naissance: Sn employeur: ahef</td>\n",
       "      <td>homme</td>\n",
       "      <td>louis-jean</td>\n",
       "      <td>louis jean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sp</td>\n",
       "      <td>chef</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           subject_line  \\\n",
       "0  ebb26ada-044c-4c62-9dbc-a9c8d505d31c   \n",
       "1  338496f5-e4ca-43ac-aa5c-429cb3f6ac00   \n",
       "\n",
       "                                                                  groundtruth  \\\n",
       "0     surname: Chardon firstname: Marie occupation: idem link: fille age: 30    \n",
       "1  surname: Lhopital firstname: Louis-Jean occupation: sp link: chef age: 67    \n",
       "\n",
       "                                                                                 prediction  \\\n",
       "0                         nom: Chardon prénom: Marie date_naissance: 30 lieux_naissance: \"    \n",
       "1  nom: Lhopital prénom: Louis Jean date_naissance: 67 lieux_naissance: Sn employeur: ahef    \n",
       "\n",
       "     sex extracted_firstname_gt extracted_firstname_pred  sex_binary  \\\n",
       "0  femme                  marie                    marie         1.0   \n",
       "1  homme             louis-jean               louis jean         0.0   \n",
       "\n",
       "  firstname  gender_probability occupation   link  \n",
       "0     marie            0.995774       idem  fille  \n",
       "1       NaN                 NaN         sp   chef  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['femme' 'homme' 'ambigu']\n"
     ]
    }
   ],
   "source": [
    "unique_sex_values = df_combined['sex'].unique()\n",
    "print(unique_sex_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex           ambigu  femme  homme\n",
      "link                              \n",
      "Chef             0.0    1.0    2.0\n",
      "Domestique       0.0    1.0    0.0\n",
      "Fils             0.0    0.0    1.0\n",
      "Leur             0.0    0.0    1.0\n",
      "Sa               0.0    2.0    0.0\n",
      "Schouer          0.0    1.0    0.0\n",
      "Son              0.0    0.0    1.0\n",
      "assisté          0.0    0.0    2.0\n",
      "belle-mère       0.0    3.0    0.0\n",
      "bru              0.0    3.0    0.0\n",
      "ch.              0.0    0.0    3.0\n",
      "chef             4.0    6.0   43.0\n",
      "domest.          0.0    1.0    0.0\n",
      "domestique       0.0    1.0    4.0\n",
      "enf              0.0    1.0    0.0\n",
      "enfant           1.0    9.0   13.0\n",
      "femme            0.0   20.0    0.0\n",
      "fille            0.0   13.0    0.0\n",
      "fils             0.0    0.0   18.0\n",
      "frère            0.0    0.0    2.0\n",
      "idem             4.0    8.0   17.0\n",
      "leur             0.0    1.0    5.0\n",
      "mère             0.0    4.0    0.0\n",
      "mére             0.0    1.0    0.0\n",
      "ouvrier          0.0    0.0    1.0\n",
      "pensionnaire     0.0    0.0    1.0\n",
      "petit-fils       0.0    0.0    2.0\n",
      "petite-fille     0.0    1.0    0.0\n",
      "père             0.0    0.0    2.0\n",
      "sa               0.0    5.0    0.0\n",
      "son              0.0    0.0    2.0\n",
      "ép               0.0    3.0    0.0\n",
      "épouse           0.0   16.0    0.0\n"
     ]
    }
   ],
   "source": [
    "link_counts = df_transcriptions.groupby('link')['sex'].value_counts().unstack().fillna(0)\n",
    "print(link_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex              ambigu  femme  homme\n",
      "occupation                           \n",
      "Cultivateur         0.0    0.0    1.0\n",
      "Domestique          0.0    1.0    0.0\n",
      "Garde               0.0    0.0    1.0\n",
      "Métayer             0.0    0.0    1.0\n",
      "Propriétaire        0.0    0.0    1.0\n",
      "Sans                0.0    1.0    0.0\n",
      "agent               0.0    0.0    1.0\n",
      "argentière          0.0    1.0    0.0\n",
      "blanchiseuse        0.0    1.0    0.0\n",
      "boulanger           0.0    1.0    0.0\n",
      "buraliste           0.0    0.0    1.0\n",
      "cantonnier          0.0    0.0    1.0\n",
      "charcutier          0.0    0.0    1.0\n",
      "charretier          0.0    0.0    1.0\n",
      "clerc               0.0    0.0    1.0\n",
      "coiffeur            0.0    0.0    1.0\n",
      "couturière          0.0    1.0    0.0\n",
      "couvreur            0.0    0.0    1.0\n",
      "culivateur          0.0    0.0    1.0\n",
      "cullotière          0.0    1.0    0.0\n",
      "cult                0.0    0.0    3.0\n",
      "cultiv              0.0    0.0    1.0\n",
      "cultivat            0.0    1.0    0.0\n",
      "cultivateur         1.0    0.0   12.0\n",
      "cultivatrice        0.0    0.0    1.0\n",
      "domest.             0.0    1.0    0.0\n",
      "domestique          0.0    4.0    5.0\n",
      "déposit             0.0    1.0    0.0\n",
      "employé             0.0    0.0    2.0\n",
      "employée            0.0    1.0    0.0\n",
      "femme               0.0    1.0    0.0\n",
      "forgeron            0.0    0.0    2.0\n",
      "garde               0.0    0.0    1.0\n",
      "garde-champêtre     0.0    0.0    1.0\n",
      "idem                6.0   23.0   25.0\n",
      "imprimeur           0.0    0.0    1.0\n",
      "jardinier           0.0    0.0    1.0\n",
      "journalier          1.0    0.0    3.0\n",
      "journalière         0.0    1.0    0.0\n",
      "manoeuvre           0.0    0.0    5.0\n",
      "menuisier           0.0    0.0    1.0\n",
      "métayer             0.0    0.0    5.0\n",
      "nourrisson          0.0    0.0    1.0\n",
      "néant               0.0   13.0    2.0\n",
      "ouvrier             0.0    0.0    2.0\n",
      "patissier           0.0    0.0    1.0\n",
      "propriétaire        0.0    1.0    0.0\n",
      "quincaillier        0.0    1.0    0.0\n",
      "receveur            0.0    0.0    1.0\n",
      "rentière            0.0    1.0    0.0\n",
      "repasseuse          0.0    1.0    0.0\n",
      "roulier             0.0    0.0    1.0\n",
      "s                   0.0    3.0    0.0\n",
      "s.p                 1.0    6.0    3.0\n",
      "s.p.                0.0    2.0    1.0\n",
      "sans                0.0   13.0    1.0\n",
      "sellier             0.0    0.0    1.0\n",
      "sp                  0.0    6.0    4.0\n",
      "tisserand           0.0    0.0    1.0\n",
      "tourneur            0.0    0.0    1.0\n",
      "voiturier           0.0    0.0    1.0\n"
     ]
    }
   ],
   "source": [
    "occupation_counts = df_transcriptions.groupby('occupation')['sex'].value_counts().unstack().fillna(0)\n",
    "print(occupation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 37\n"
     ]
    }
   ],
   "source": [
    "# number of nan values in the firstname column\n",
    "print(df_combined['firstname'].isna().sum(),df_combined_pred['firstname'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               male        female         total  gender_probability\n",
      "count  6.946000e+03  6.946000e+03  6.946000e+03         6946.000000\n",
      "mean   2.084710e+03  1.795356e+03  3.880067e+03            0.510974\n",
      "std    3.725168e+04  3.738858e+04  5.290102e+04            0.477295\n",
      "min    0.000000e+00  0.000000e+00  1.000000e+01            0.000000\n",
      "25%    0.000000e+00  0.000000e+00  1.600000e+01            0.000000\n",
      "50%    1.100000e+01  1.200000e+01  3.500000e+01            0.631579\n",
      "75%    4.000000e+01  4.300000e+01  1.450000e+02            1.000000\n",
      "max    1.869615e+06  2.390322e+06  2.400467e+06            1.000000\n",
      "Pourcentage de prénoms masculins: 53.73%\n",
      "Pourcentage de prénoms féminins: 46.27%\n"
     ]
    }
   ],
   "source": [
    "descriptive_stats = df_prenoms.describe()\n",
    "\n",
    "# Proportion of each sex \n",
    "total_male = df_prenoms['male'].sum()\n",
    "total_female = df_prenoms['female'].sum()\n",
    "total = total_male + total_female\n",
    "\n",
    "male_percentage = (total_male / total) * 100\n",
    "female_percentage = (total_female / total) * 100\n",
    "\n",
    "\n",
    "print(descriptive_stats)\n",
    "\n",
    "print(f\"Pourcentage de prénoms masculins: {male_percentage:.2f}%\")\n",
    "print(f\"Pourcentage de prénoms féminins: {female_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14480397 12470545\n"
     ]
    }
   ],
   "source": [
    "print(total_male,total_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First predictions :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with the clean data : using only the probability of the first name being in one gender (ground truth data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.9543568464730291\n"
     ]
    }
   ],
   "source": [
    "# Predict gender based on gender probability\n",
    "df_combined['predicted_gender_proba_only'] = df_combined['gender_probability'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (df_combined['predicted_gender_proba_only'] == df_combined['sex_binary']).mean()\n",
    "print(\"Baseline accuracy:\", accuracy)\n",
    "\n",
    "# Accuracy is 0.95 with the baseline. It is a good results but we have to keep in mind that the data is clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03734439834024896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_combined['sex_binary'].isna().sum()/len(df_combined))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.9045643153526971\n"
     ]
    }
   ],
   "source": [
    "# Predict gender based on gender probability\n",
    "df_combined_pred['predicted_gender_proba_only'] = df_combined_pred['gender_probability'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (df_combined_pred['predicted_gender_proba_only'] == df_combined_pred['sex_binary']).mean()\n",
    "print(\"Baseline accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 90% accuraccy on the prediction data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.8049792531120332\n"
     ]
    }
   ],
   "source": [
    "# Predict gender based on gender probability\n",
    "df_combined_pred['predicted_gender_proba_only'] = df_combined_pred['gender_probability'].apply(lambda x: 1 if x > 0.5 else (0 if x <= 0.5 else x))\n",
    "# Compute accuracy\n",
    "accuracy = (df_combined_pred['predicted_gender_proba_only'] == df_combined_pred['sex_binary']).mean()\n",
    "print(\"Baseline accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take into account the links and occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lists for male and female links and occupations\n",
    "male_links = ['Son', 'Fils', 'Père', 'Frère','Chef']\n",
    "female_links = ['Fille', 'Fils', 'Mère', 'Soeur']\n",
    "#male_occupations = ['Chef'] # men were the only chef of the house at the time of the dataset\n",
    "#female_occupations = ['Femme']\n",
    "\n",
    "# Define a function to determine the predicted gender based on link and occupation\n",
    "'''\n",
    "def predict_gender_gt(row):\n",
    "    if row['link'] in male_links:\n",
    "        return 0\n",
    "    elif row['link'] in female_links:\n",
    "        return 1\n",
    "#    elif row['occupation'] in male_occupations:\n",
    "#        return 0\n",
    "#    elif row['occupation'] in female_occupations:\n",
    "#        return 1\n",
    "    else:\n",
    "        return 1 if row['gender_probability'] > 0.5\n",
    "'''\n",
    "def predict_gender_gt(row):\n",
    "    if row['link'] in male_links:\n",
    "        return 0\n",
    "    elif row['link'] in female_links:\n",
    "        return 1\n",
    "    else:\n",
    "        # Check if the value is NaN\n",
    "        if np.isnan(row['gender_probability']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return 1 if row['gender_probability'] > 0.5 else 0\n",
    "    \n",
    "'''    \n",
    "def predict_gender_pred(row):\n",
    "    if row['relation'] in male_links:\n",
    "        return 0\n",
    "    elif row['relation'] in female_links:\n",
    "        return 1\n",
    "#    elif row['profession'] in male_occupations:\n",
    "#        return 0\n",
    "#    elif row['profession'] in female_occupations:\n",
    "#        return 1\n",
    "    else:\n",
    "        return 1 if row['gender_probability'] > 0.5 else 0    \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def predict_gender_pred(row):\n",
    "    if row['relation'] in male_links:\n",
    "        return 0\n",
    "    elif row['relation'] in female_links:\n",
    "        return 1\n",
    "    \n",
    "    # elif row['profession'] in male_occupations:\n",
    "    #     return 0\n",
    "    # elif row['profession'] in female_occupations:\n",
    "    #     return 1\n",
    "    else:\n",
    "        if np.isnan(row['gender_probability']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return 1 if row['gender_probability'] > 0.5 else 0\n",
    "\n",
    "\n",
    "\n",
    "# Create the predicted_gender_all_data column\n",
    "df_combined['predicted_gender_all_data'] = df_combined.apply(predict_gender_gt, axis=1)\n",
    "df_combined_pred['predicted_gender_all_data'] = df_combined_pred.apply(predict_gender_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8008298755186722\n"
     ]
    }
   ],
   "source": [
    "accuracy = (df_combined_pred['predicted_gender_all_data'] == df_combined_pred['sex_binary']).mean()\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#Accuracy of 80% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.946078431372549\n"
     ]
    }
   ],
   "source": [
    "df_combined_pred_no_nan = df_combined_pred.dropna(subset=['gender_probability'])\n",
    "\n",
    "accuracy = (df_combined_pred_no_nan['predicted_gender_all_data'] == df_combined_pred_no_nan['sex_binary']).mean()\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#Accuracy of 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try NLP models ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero shot on ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Error while downloading from https://cdn-lfs.huggingface.co/facebook/bart-large-mnli/cfbb687dbbd9df99fe865e1860350a22aebac4d26ee4bcb50217f1df606a018e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1712580526&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMjU4MDUyNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLW1ubGkvY2ZiYjY4N2RiYmQ5ZGY5OWZlODY1ZTE4NjAzNTBhMjJhZWJhYzRkMjZlZTRiY2I1MDIxN2YxZGY2MDZhMDE4ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=hiTC6XAVmspAo9BeEaFG0kBxjbMieLCIVOBQw4au3XiRi%7EYHlsVNt37hRH7PXZneMDgj1tQP33hNkyxePSBnygWd3Kh8aytfiTYBYuJ5SzrzJI6Syr-jSr5YeM5O0AYC2Gw7XIrlrR43%7Er-ew0LJoU9rlBMZZBhWZS%7E1mq5IcTvvS65q0XlN9togrd-Ek5WSFqfCdEFyNUR7ycRc0Mnk4PABaeuM944J6nOYUpau-hN6xHayQ7-ZResYp%7EwAk6pACmmpX9iXxMMbNk7NdWc6nN01UCkQoK98nduTCnk8vNXrAHpV5mGEYjgJsd%7EWHzaK1J5nr%7EKBCK9lkhtbK0uK7Q__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.huggingface.co/facebook/bart-large-mnli/cfbb687dbbd9df99fe865e1860350a22aebac4d26ee4bcb50217f1df606a018e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1712583406&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMjU4MzQwNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLW1ubGkvY2ZiYjY4N2RiYmQ5ZGY5OWZlODY1ZTE4NjAzNTBhMjJhZWJhYzRkMjZlZTRiY2I1MDIxN2YxZGY2MDZhMDE4ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=vI67kec9cRWvC-2B3UHQznDDGWBn6N367Tl1%7EUMVa8GUT0575UQFsfJlRk5KWbzOnYK%7EN%7E7OrtWVj-pj%7EJBmvrxqzS9F80mZrJ2UmszEgcjILS6kn3dBUvgETiRr5qljwuzfiDOm9etD8Y4tElM7sIDruCm%7ENZoeI0stO1kMW5BRiKsZCk13TtK2kV08%7EZ8QGvEEON9CsgzPkYvasAqSS2BQxiwvybK5mPD4BkCGhcdXK8ctNfWSJZjTr5xDa09MwZyj9sbsrdBQwvrskWz5XmrNiXiO3dGBN7pR8BDgUDo8EYXA%7EdLnHWH3WYcnMu0Ce-aF4noTKzfvZdKVbYfAzQ__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-large-mnli and are newly initialized: ['model.decoder.embed_tokens.weight', 'model.encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' can be long to run\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Define candidate labels\n",
    "labels = [\"male\", \"female\"]\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for text in df_combined['groundtruth']:\n",
    "    # Predict the label of the text\n",
    "    result = classifier(text, labels)\n",
    "    # Get the label with the highest score\n",
    "    predicted_label = result[\"labels\"][result[\"scores\"].index(max(result[\"scores\"]))]\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "df_combined['predicted_sex_zero_shot'] = predictions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9377593360995851"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_combined['predicted_sex_zero_shot_binary'] = df_combined['predicted_sex_zero_shot'].apply(lambda x: 1 if x == 'female' else 0)\n",
    "\n",
    "accuracy_zero_shot = (df_combined['predicted_sex_zero_shot_binary'] == df_combined['sex_binary']).mean()\n",
    "accuracy_zero_shot\n",
    "\n",
    "# 0.937759% accuracy with zero shot classification\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-large-mnli and are newly initialized: ['model.decoder.embed_tokens.weight', 'model.encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Define candidate labels\n",
    "labels = [\"male\", \"female\"]\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for text in df_combined['prediction']:\n",
    "    # Predict the label of the text\n",
    "    result = classifier(text, labels)\n",
    "    # Get the label with the highest score\n",
    "    predicted_label = result[\"labels\"][result[\"scores\"].index(max(result[\"scores\"]))]\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "df_combined_pred['predicted_sex_zero_shot'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8672199170124482"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_pred['predicted_sex_zero_shot_binary'] = df_combined_pred['predicted_sex_zero_shot'].apply(lambda x: 1 if x == 'female' else 0)\n",
    "\n",
    "accuracy_zero_shot = (df_combined_pred['predicted_sex_zero_shot_binary'] == df_combined['sex_binary']).mean()\n",
    "accuracy_zero_shot\n",
    "\n",
    "#accuracy of 0.8672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.995774\n",
       "1           NaN\n",
       "2      0.995774\n",
       "3      0.995774\n",
       "4      0.998233\n",
       "         ...   \n",
       "236    0.997709\n",
       "237    0.004556\n",
       "238    0.998233\n",
       "239    0.004232\n",
       "240    0.997834\n",
       "Name: gender_probability, Length: 241, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['gender_probability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model on cases that are impossible for the rule-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when probability is NaN: 0.6486486486486487\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to include only rows where the probability column is NaN\n",
    "df_filtered = df_combined_pred[df_combined_pred['gender_probability'].isna()]\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (df_filtered['predicted_gender_all_data'] == df_filtered['sex_binary']).mean()\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy when probability is NaN:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 13)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9005560165975103\n"
     ]
    }
   ],
   "source": [
    "# Accuracy des modèles combinés\n",
    "# accuracy = weighted average of the accuracy of the two models\n",
    "Accuracy_combined = (65/100)*(37/241) + (94.6/100)*(204/241)\n",
    "print(Accuracy_combined)\n",
    "\n",
    "# Accuracy of 90,23% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonyivanier/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenisation et préparation des données pour le modèle\n",
    "class GenderPredictionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        \n",
    "        # Important: Convert 'BatchEncoding' to a more usable format\n",
    "        input_ids = encoding['input_ids'].squeeze()  \n",
    "        attention_mask = encoding['attention_mask'].squeeze() \n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': torch.tensor(labels)}\n",
    "\n",
    "\n",
    "texts = df_combined['prediction'].tolist()\n",
    "labels = df_combined['sex_binary'].tolist()\n",
    "\n",
    "# Division in train and test sets\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Datasets creation\n",
    "train_dataset = GenderPredictionDataset(texts_train, labels_train, tokenizer)\n",
    "test_dataset = GenderPredictionDataset(texts_test, labels_test, tokenizer)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/anthonyivanier/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 192\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 72\n",
      "  Number of trainable parameters = 66955010\n",
      " 33%|███▎      | 24/72 [01:23<02:44,  3.44s/it]***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 8\n",
      "                                               \n",
      " 33%|███▎      | 24/72 [01:29<02:44,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6939376592636108, 'eval_runtime': 5.6103, 'eval_samples_per_second': 8.734, 'eval_steps_per_second': 1.248, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 48/72 [02:51<01:22,  3.43s/it]***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 8\n",
      "                                               \n",
      " 67%|██████▋   | 48/72 [02:57<01:22,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6911244988441467, 'eval_runtime': 5.6169, 'eval_samples_per_second': 8.724, 'eval_steps_per_second': 1.246, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [04:21<00:00,  3.53s/it]***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 8\n",
      "                                               \n",
      "100%|██████████| 72/72 [04:26<00:00,  3.53s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 72/72 [04:26<00:00,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6831199526786804, 'eval_runtime': 5.6784, 'eval_samples_per_second': 8.629, 'eval_steps_per_second': 1.233, 'epoch': 3.0}\n",
      "{'train_runtime': 266.9419, 'train_samples_per_second': 2.158, 'train_steps_per_second': 0.27, 'train_loss': 0.6858603689405653, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=0.6858603689405653, metrics={'train_runtime': 266.9419, 'train_samples_per_second': 2.158, 'train_steps_per_second': 0.27, 'train_loss': 0.6858603689405653, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 49\n",
      "  Batch size = 8\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6831199526786804, 'eval_runtime': 5.811, 'eval_samples_per_second': 8.432, 'eval_steps_per_second': 1.205, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "evaluation_results = trainer.evaluate(test_dataset)\n",
    "print(evaluation_results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 49\n",
      "  Batch size = 8\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.47it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "cm = confusion_matrix(labels_test, predicted_labels)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_no_nan = df_combined.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenization and data preparation for the model\n",
    "class GenderPredictionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = int(self.labels[idx])  # Convert label to integer\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        \n",
    "        \n",
    "        label_one_hot = torch.zeros(2)\n",
    "        label_one_hot[label] = 1\n",
    "\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()  \n",
    "        attention_mask = encoding['attention_mask'].squeeze() \n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label_one_hot}\n",
    "\n",
    "\n",
    "texts = df_combined_no_nan['prediction'].tolist()\n",
    "labels = df_combined_no_nan['sex_binary'].tolist()\n",
    "\n",
    "\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = GenderPredictionDataset(texts_train, labels_train, tokenizer)\n",
    "test_dataset = GenderPredictionDataset(texts_test, labels_test, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/anthonyivanier/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 153\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "  Number of trainable parameters = 66955010\n",
      " 10%|█         | 20/200 [01:05<07:36,  2.53s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 10%|█         | 20/200 [01:07<07:36,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7040766477584839, 'eval_runtime': 1.952, 'eval_samples_per_second': 9.221, 'eval_steps_per_second': 1.537, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [02:12<06:44,  2.53s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 20%|██        | 40/200 [02:14<06:44,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7040635347366333, 'eval_runtime': 1.9309, 'eval_samples_per_second': 9.322, 'eval_steps_per_second': 1.554, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 60/200 [03:20<06:02,  2.59s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 30%|███       | 60/200 [03:22<06:02,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7057966589927673, 'eval_runtime': 1.9907, 'eval_samples_per_second': 9.042, 'eval_steps_per_second': 1.507, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80/200 [04:29<05:17,  2.65s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 40%|████      | 80/200 [04:31<05:17,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7064812779426575, 'eval_runtime': 2.0379, 'eval_samples_per_second': 8.833, 'eval_steps_per_second': 1.472, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [05:39<04:23,  2.64s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 50%|█████     | 100/200 [05:41<04:23,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.705090343952179, 'eval_runtime': 2.0305, 'eval_samples_per_second': 8.865, 'eval_steps_per_second': 1.477, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 120/200 [06:49<03:33,  2.67s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 60%|██████    | 120/200 [06:51<03:33,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6955525875091553, 'eval_runtime': 2.0581, 'eval_samples_per_second': 8.746, 'eval_steps_per_second': 1.458, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 140/200 [08:00<02:39,  2.66s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 70%|███████   | 140/200 [08:02<02:39,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6962171196937561, 'eval_runtime': 2.0671, 'eval_samples_per_second': 8.708, 'eval_steps_per_second': 1.451, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160/200 [09:11<01:46,  2.67s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 80%|████████  | 160/200 [09:13<01:46,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6626589298248291, 'eval_runtime': 2.097, 'eval_samples_per_second': 8.584, 'eval_steps_per_second': 1.431, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180/200 [10:22<00:53,  2.68s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 90%|█████████ | 180/200 [10:24<00:53,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.54349684715271, 'eval_runtime': 2.0783, 'eval_samples_per_second': 8.661, 'eval_steps_per_second': 1.443, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:33<00:00,  2.68s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      "100%|██████████| 200/200 [11:35<00:00,  2.68s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 200/200 [11:35<00:00,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43833765387535095, 'eval_runtime': 2.0713, 'eval_samples_per_second': 8.69, 'eval_steps_per_second': 1.448, 'epoch': 10.0}\n",
      "{'train_runtime': 695.2136, 'train_samples_per_second': 2.201, 'train_steps_per_second': 0.288, 'train_loss': 0.6505979919433593, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.6505979919433593, metrics={'train_runtime': 695.2136, 'train_samples_per_second': 2.201, 'train_steps_per_second': 0.288, 'train_loss': 0.6505979919433593, 'epoch': 10.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from transformers import AdamW\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,  # Increase the number of epochs\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1            #1e-5  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-5), None)  # Use AdamW optimizer\n",
    ")\n",
    "\n",
    "trainer.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/anthonyivanier/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 153\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "  Number of trainable parameters = 66955010\n",
      "  2%|▎         | 20/800 [01:06<33:13,  2.56s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      "  2%|▎         | 20/800 [01:08<33:13,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7167547941207886, 'eval_runtime': 1.9542, 'eval_samples_per_second': 9.211, 'eval_steps_per_second': 1.535, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 40/800 [02:15<33:29,  2.64s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      "  5%|▌         | 40/800 [02:18<33:29,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7162106037139893, 'eval_runtime': 2.0549, 'eval_samples_per_second': 8.76, 'eval_steps_per_second': 1.46, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 60/800 [03:27<34:13,  2.78s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      "  8%|▊         | 60/800 [03:29<34:13,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7180238962173462, 'eval_runtime': 2.218, 'eval_samples_per_second': 8.115, 'eval_steps_per_second': 1.353, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 80/800 [04:40<33:31,  2.79s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 10%|█         | 80/800 [04:42<33:31,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7172243595123291, 'eval_runtime': 2.0801, 'eval_samples_per_second': 8.653, 'eval_steps_per_second': 1.442, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 100/800 [05:52<31:24,  2.69s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 12%|█▎        | 100/800 [05:54<31:24,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7126942873001099, 'eval_runtime': 2.093, 'eval_samples_per_second': 8.6, 'eval_steps_per_second': 1.433, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 120/800 [07:03<30:34,  2.70s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 15%|█▌        | 120/800 [07:05<30:34,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7018867135047913, 'eval_runtime': 2.078, 'eval_samples_per_second': 8.662, 'eval_steps_per_second': 1.444, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 140/800 [08:14<29:46,  2.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 18%|█▊        | 140/800 [08:16<29:46,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7050731182098389, 'eval_runtime': 2.1041, 'eval_samples_per_second': 8.555, 'eval_steps_per_second': 1.426, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 160/800 [09:27<28:39,  2.69s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 20%|██        | 160/800 [09:29<28:39,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.677053689956665, 'eval_runtime': 2.11, 'eval_samples_per_second': 8.531, 'eval_steps_per_second': 1.422, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 180/800 [10:38<27:58,  2.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 22%|██▎       | 180/800 [10:40<27:58,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.599799394607544, 'eval_runtime': 2.1224, 'eval_samples_per_second': 8.481, 'eval_steps_per_second': 1.414, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 200/800 [11:50<27:06,  2.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 25%|██▌       | 200/800 [11:52<27:06,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4863348603248596, 'eval_runtime': 2.1053, 'eval_samples_per_second': 8.55, 'eval_steps_per_second': 1.425, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 220/800 [13:02<26:08,  2.70s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 28%|██▊       | 220/800 [13:04<26:08,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40958163142204285, 'eval_runtime': 2.1101, 'eval_samples_per_second': 8.531, 'eval_steps_per_second': 1.422, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 240/800 [14:13<25:10,  2.70s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 30%|███       | 240/800 [14:16<25:10,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3511832058429718, 'eval_runtime': 2.0977, 'eval_samples_per_second': 8.581, 'eval_steps_per_second': 1.43, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 260/800 [15:26<24:24,  2.71s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 32%|███▎      | 260/800 [15:28<24:24,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28498753905296326, 'eval_runtime': 2.1312, 'eval_samples_per_second': 8.446, 'eval_steps_per_second': 1.408, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 280/800 [16:38<23:38,  2.73s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 35%|███▌      | 280/800 [16:40<23:38,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27359867095947266, 'eval_runtime': 2.1352, 'eval_samples_per_second': 8.43, 'eval_steps_per_second': 1.405, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 300/800 [17:50<22:39,  2.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 38%|███▊      | 300/800 [17:52<22:39,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24379508197307587, 'eval_runtime': 2.1255, 'eval_samples_per_second': 8.469, 'eval_steps_per_second': 1.411, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 320/800 [19:02<21:46,  2.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 40%|████      | 320/800 [19:04<21:46,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22216421365737915, 'eval_runtime': 2.1254, 'eval_samples_per_second': 8.469, 'eval_steps_per_second': 1.411, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 340/800 [20:15<21:02,  2.74s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 42%|████▎     | 340/800 [20:17<21:02,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25499045848846436, 'eval_runtime': 2.1599, 'eval_samples_per_second': 8.334, 'eval_steps_per_second': 1.389, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 360/800 [21:28<20:02,  2.73s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 45%|████▌     | 360/800 [21:30<20:02,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23757094144821167, 'eval_runtime': 2.1615, 'eval_samples_per_second': 8.328, 'eval_steps_per_second': 1.388, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 380/800 [22:40<19:10,  2.74s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 48%|████▊     | 380/800 [22:42<19:10,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24088886380195618, 'eval_runtime': 2.1336, 'eval_samples_per_second': 8.437, 'eval_steps_per_second': 1.406, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 400/800 [23:56<19:44,  2.96s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 50%|█████     | 400/800 [23:58<19:44,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23501703143119812, 'eval_runtime': 2.1337, 'eval_samples_per_second': 8.436, 'eval_steps_per_second': 1.406, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 420/800 [25:11<17:50,  2.82s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 52%|█████▎    | 420/800 [25:14<17:50,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24698257446289062, 'eval_runtime': 2.2495, 'eval_samples_per_second': 8.002, 'eval_steps_per_second': 1.334, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 440/800 [26:26<16:29,  2.75s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 55%|█████▌    | 440/800 [26:28<16:29,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2517710328102112, 'eval_runtime': 2.1113, 'eval_samples_per_second': 8.526, 'eval_steps_per_second': 1.421, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 460/800 [27:39<15:39,  2.76s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 57%|█████▊    | 460/800 [27:41<15:39,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3935675323009491, 'eval_runtime': 2.1076, 'eval_samples_per_second': 8.541, 'eval_steps_per_second': 1.423, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 480/800 [28:53<14:52,  2.79s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 60%|██████    | 480/800 [28:55<14:52,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26184654235839844, 'eval_runtime': 2.2029, 'eval_samples_per_second': 8.171, 'eval_steps_per_second': 1.362, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 500/800 [30:06<13:37,  2.72s/it]Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3376, 'learning_rate': 1e-05, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 62%|██████▎   | 500/800 [30:09<13:37,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27712732553482056, 'eval_runtime': 2.2885, 'eval_samples_per_second': 7.865, 'eval_steps_per_second': 1.311, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 520/800 [31:22<13:03,  2.80s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 65%|██████▌   | 520/800 [31:24<13:03,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27706262469291687, 'eval_runtime': 2.161, 'eval_samples_per_second': 8.33, 'eval_steps_per_second': 1.388, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 540/800 [32:36<11:56,  2.76s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 68%|██████▊   | 540/800 [32:38<11:56,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27701297402381897, 'eval_runtime': 2.1459, 'eval_samples_per_second': 8.388, 'eval_steps_per_second': 1.398, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 560/800 [33:49<11:12,  2.80s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 70%|███████   | 560/800 [33:52<11:12,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31513944268226624, 'eval_runtime': 2.4606, 'eval_samples_per_second': 7.315, 'eval_steps_per_second': 1.219, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 580/800 [35:03<10:01,  2.74s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 72%|███████▎  | 580/800 [35:05<10:01,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33124208450317383, 'eval_runtime': 2.127, 'eval_samples_per_second': 8.463, 'eval_steps_per_second': 1.41, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 600/800 [36:15<09:04,  2.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 75%|███████▌  | 600/800 [36:17<09:04,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28682729601860046, 'eval_runtime': 2.0937, 'eval_samples_per_second': 8.597, 'eval_steps_per_second': 1.433, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 620/800 [37:28<08:16,  2.76s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 78%|███████▊  | 620/800 [37:30<08:16,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2877143919467926, 'eval_runtime': 2.1142, 'eval_samples_per_second': 8.514, 'eval_steps_per_second': 1.419, 'epoch': 31.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 640/800 [38:40<07:15,  2.72s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 80%|████████  | 640/800 [38:42<07:15,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2940234839916229, 'eval_runtime': 2.1117, 'eval_samples_per_second': 8.524, 'eval_steps_per_second': 1.421, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 660/800 [39:52<06:25,  2.76s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 82%|████████▎ | 660/800 [39:54<06:25,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29434478282928467, 'eval_runtime': 2.1252, 'eval_samples_per_second': 8.47, 'eval_steps_per_second': 1.412, 'epoch': 33.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 680/800 [41:08<05:47,  2.89s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 85%|████████▌ | 680/800 [41:10<05:47,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2956094741821289, 'eval_runtime': 2.1404, 'eval_samples_per_second': 8.41, 'eval_steps_per_second': 1.402, 'epoch': 34.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 700/800 [42:24<04:45,  2.85s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 88%|████████▊ | 700/800 [42:26<04:45,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29744240641593933, 'eval_runtime': 2.1584, 'eval_samples_per_second': 8.339, 'eval_steps_per_second': 1.39, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 720/800 [43:39<03:44,  2.81s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 90%|█████████ | 720/800 [43:41<03:44,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.298817902803421, 'eval_runtime': 2.1098, 'eval_samples_per_second': 8.532, 'eval_steps_per_second': 1.422, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 740/800 [44:55<02:53,  2.90s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 92%|█████████▎| 740/800 [44:57<02:53,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3009965419769287, 'eval_runtime': 2.1557, 'eval_samples_per_second': 8.35, 'eval_steps_per_second': 1.392, 'epoch': 37.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 760/800 [46:11<01:53,  2.85s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 95%|█████████▌| 760/800 [46:13<01:53,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30175095796585083, 'eval_runtime': 2.1069, 'eval_samples_per_second': 8.544, 'eval_steps_per_second': 1.424, 'epoch': 38.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 780/800 [47:26<00:58,  2.90s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      " 98%|█████████▊| 780/800 [47:28<00:58,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3022264838218689, 'eval_runtime': 2.1993, 'eval_samples_per_second': 8.184, 'eval_steps_per_second': 1.364, 'epoch': 39.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [48:42<00:00,  2.92s/it]***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "\n",
      "100%|██████████| 800/800 [48:44<00:00,  2.92s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 800/800 [48:44<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30253615975379944, 'eval_runtime': 2.2613, 'eval_samples_per_second': 7.96, 'eval_steps_per_second': 1.327, 'epoch': 40.0}\n",
      "{'train_runtime': 2924.5819, 'train_samples_per_second': 2.093, 'train_steps_per_second': 0.274, 'train_loss': 0.21434829950332643, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=0.21434829950332643, metrics={'train_runtime': 2924.5819, 'train_samples_per_second': 2.093, 'train_steps_per_second': 0.274, 'train_loss': 0.21434829950332643, 'epoch': 40.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=40,  # Increase the number of epochs\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1            #1e-5  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-5), None)  # Use AdamW optimizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "100%|██████████| 11/11 [00:08<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6905139088630676, 'eval_runtime': 9.1998, 'eval_samples_per_second': 9.348, 'eval_steps_per_second': 1.196, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate(test_dataset)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/anthonyivanier/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 18\n",
      "  Batch size = 64\n",
      "100%|██████████| 1/1 [00:00<00:00, 4760.84it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=40,  # Increase the number of epochs\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1            #1e-5  \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-5), None)  # Use AdamW optimizer\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "predicted_labels = predictions.predictions.argmax(axis=0)\n",
    "\n",
    "own_predicted_labels = np.where(predictions.predictions[:, 0] > predictions.predictions[:, 1], 0, 1)\n",
    "\n",
    "#np.where(predictions[:, 0] > predictions[:, 1], 0, 1)\n",
    "\n",
    "cm = confusion_matrix(labels_test, own_predicted_labels)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-4.952337 ,  5.0493884],\n",
       "       [-2.394292 ,  2.3718576],\n",
       "       [-5.0438075,  5.156636 ],\n",
       "       [-4.724189 ,  4.8422427],\n",
       "       [ 5.2602015, -5.2494845],\n",
       "       [-5.040811 ,  5.1767216],\n",
       "       [-5.0134077,  5.1311526],\n",
       "       [-5.0299215,  5.1747656],\n",
       "       [ 5.3380218, -5.318031 ],\n",
       "       [ 5.332298 , -5.296445 ],\n",
       "       [ 5.30994  , -5.320905 ],\n",
       "       [-5.0388427,  5.1214676],\n",
       "       [-4.9438953,  5.035487 ],\n",
       "       [ 5.2795258, -5.2630005],\n",
       "       [-5.0145636,  5.161049 ],\n",
       "       [-5.0178995,  5.141307 ],\n",
       "       [-4.8227553,  4.8959312],\n",
       "       [ 5.3120704, -5.3124766]], dtype=float32), label_ids=array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32), metrics={'test_loss': 0.30253615975379944, 'test_runtime': 2.1539, 'test_samples_per_second': 8.357, 'test_steps_per_second': 1.393})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43 76]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert few-shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: torch in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: fsspec in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: networkx in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: filelock in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: sympy in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/anthonyivanier/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/anthonyivanier/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      0.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      1.0\n",
       "5      0.0\n",
       "6      0.0\n",
       "7      0.0\n",
       "8      1.0\n",
       "9      1.0\n",
       "10     0.0\n",
       "11     1.0\n",
       "12     1.0\n",
       "13     1.0\n",
       "14     0.0\n",
       "15     0.0\n",
       "16     1.0\n",
       "17     NaN\n",
       "18     0.0\n",
       "19     1.0\n",
       "20     1.0\n",
       "21     0.0\n",
       "22     1.0\n",
       "23     1.0\n",
       "24     0.0\n",
       "25     1.0\n",
       "26     0.0\n",
       "27     1.0\n",
       "28     0.0\n",
       "29     0.0\n",
       "30     0.0\n",
       "31     1.0\n",
       "32     0.0\n",
       "33     0.0\n",
       "34     1.0\n",
       "35     0.0\n",
       "36     0.0\n",
       "37     1.0\n",
       "38     0.0\n",
       "39     1.0\n",
       "40     1.0\n",
       "41     1.0\n",
       "42     1.0\n",
       "43     0.0\n",
       "44     0.0\n",
       "45     1.0\n",
       "46     0.0\n",
       "47     1.0\n",
       "48     1.0\n",
       "49     0.0\n",
       "50     0.0\n",
       "51     1.0\n",
       "52     0.0\n",
       "53     1.0\n",
       "54     1.0\n",
       "55     1.0\n",
       "56     1.0\n",
       "57     1.0\n",
       "58     0.0\n",
       "59     NaN\n",
       "60     0.0\n",
       "61     1.0\n",
       "62     0.0\n",
       "63     0.0\n",
       "64     1.0\n",
       "65     0.0\n",
       "66     1.0\n",
       "67     1.0\n",
       "68     1.0\n",
       "69     0.0\n",
       "70     0.0\n",
       "71     1.0\n",
       "72     0.0\n",
       "73     0.0\n",
       "74     1.0\n",
       "75     0.0\n",
       "76     0.0\n",
       "77     1.0\n",
       "78     1.0\n",
       "79     1.0\n",
       "80     1.0\n",
       "81     1.0\n",
       "82     0.0\n",
       "83     0.0\n",
       "84     0.0\n",
       "85     0.0\n",
       "86     1.0\n",
       "87     1.0\n",
       "88     0.0\n",
       "89     1.0\n",
       "90     0.0\n",
       "91     0.0\n",
       "92     0.0\n",
       "93     1.0\n",
       "94     NaN\n",
       "95     0.0\n",
       "96     0.0\n",
       "97     1.0\n",
       "98     0.0\n",
       "99     1.0\n",
       "100    0.0\n",
       "101    0.0\n",
       "102    0.0\n",
       "103    1.0\n",
       "104    0.0\n",
       "105    0.0\n",
       "106    1.0\n",
       "107    0.0\n",
       "108    1.0\n",
       "109    1.0\n",
       "110    0.0\n",
       "111    0.0\n",
       "112    0.0\n",
       "113    1.0\n",
       "114    1.0\n",
       "115    0.0\n",
       "116    0.0\n",
       "117    1.0\n",
       "118    0.0\n",
       "119    0.0\n",
       "120    1.0\n",
       "121    0.0\n",
       "122    1.0\n",
       "123    NaN\n",
       "124    NaN\n",
       "125    0.0\n",
       "126    0.0\n",
       "127    0.0\n",
       "128    0.0\n",
       "129    0.0\n",
       "130    NaN\n",
       "131    0.0\n",
       "132    1.0\n",
       "133    0.0\n",
       "134    1.0\n",
       "135    1.0\n",
       "136    0.0\n",
       "137    1.0\n",
       "138    1.0\n",
       "139    0.0\n",
       "140    0.0\n",
       "141    1.0\n",
       "142    0.0\n",
       "143    0.0\n",
       "144    0.0\n",
       "145    0.0\n",
       "146    0.0\n",
       "147    0.0\n",
       "148    1.0\n",
       "149    1.0\n",
       "150    1.0\n",
       "151    1.0\n",
       "152    0.0\n",
       "153    0.0\n",
       "154    1.0\n",
       "155    0.0\n",
       "156    0.0\n",
       "157    1.0\n",
       "158    0.0\n",
       "159    0.0\n",
       "160    0.0\n",
       "161    0.0\n",
       "162    1.0\n",
       "163    1.0\n",
       "164    0.0\n",
       "165    1.0\n",
       "166    1.0\n",
       "167    0.0\n",
       "168    1.0\n",
       "169    1.0\n",
       "170    0.0\n",
       "171    0.0\n",
       "172    0.0\n",
       "173    1.0\n",
       "174    0.0\n",
       "175    1.0\n",
       "176    1.0\n",
       "177    1.0\n",
       "178    1.0\n",
       "179    0.0\n",
       "180    0.0\n",
       "181    1.0\n",
       "182    1.0\n",
       "183    1.0\n",
       "184    1.0\n",
       "185    0.0\n",
       "186    1.0\n",
       "187    1.0\n",
       "188    0.0\n",
       "189    0.0\n",
       "190    1.0\n",
       "191    1.0\n",
       "192    0.0\n",
       "193    1.0\n",
       "194    0.0\n",
       "195    1.0\n",
       "196    NaN\n",
       "197    1.0\n",
       "198    0.0\n",
       "199    0.0\n",
       "200    0.0\n",
       "201    0.0\n",
       "202    0.0\n",
       "203    0.0\n",
       "204    NaN\n",
       "205    0.0\n",
       "206    0.0\n",
       "207    0.0\n",
       "208    1.0\n",
       "209    1.0\n",
       "210    1.0\n",
       "211    0.0\n",
       "212    0.0\n",
       "213    1.0\n",
       "214    1.0\n",
       "215    0.0\n",
       "216    0.0\n",
       "217    1.0\n",
       "218    0.0\n",
       "219    1.0\n",
       "220    0.0\n",
       "221    1.0\n",
       "222    0.0\n",
       "223    0.0\n",
       "224    0.0\n",
       "225    0.0\n",
       "226    0.0\n",
       "227    0.0\n",
       "228    0.0\n",
       "229    1.0\n",
       "230    NaN\n",
       "231    1.0\n",
       "232    0.0\n",
       "233    0.0\n",
       "234    0.0\n",
       "235    1.0\n",
       "236    1.0\n",
       "237    0.0\n",
       "238    1.0\n",
       "239    0.0\n",
       "240    1.0\n",
       "Name: sex_binary, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['sex_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_texts = [\n",
    "    \"nom: Chardon prénom: Marie date_naissance: 30 lieux_naissance: \",\n",
    "    \"nom: Lhopital prénom: Louis Jean date_naissance: 67 lieux_naissance: Sn employeur: ahef\",\n",
    "    \"nom: Pyrin prénom: Marie date_naissance: 55 relation: d\",\n",
    "    \"nom: Roy prénom: Antoine date_naissance: 51 lieux_naissance: P profession: métro\"\n",
    "]\n",
    "\n",
    "# Convert labels to integers\n",
    "label_mapping = {\"Femme\": 0, \"Homme\": 1}\n",
    "train_labels = [label_mapping[label] for label in [\"Femme\", \"Homme\", \"Femme\", \"Homme\"]]\n",
    "df_combined['sex_binary_int'] = df_combined['sex_binary'].astype(int)\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_texts, train_labels, tokenizer)\n",
    "val_texts = df_combined['prediction'].tolist()\n",
    "val_labels = df_combined['sex_binary_int'].tolist()\n",
    "\n",
    "val_dataset = MyDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_dataset = MyDataset(train_texts, train_labels, tokenizer)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/ll4cqtvn3yx5kfcbb8jfz_p40000gn/T/ipykernel_5652/1096317986.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_combined_no_nan['sex_binary_int'] = df_combined_no_nan['sex_binary'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "train_texts = [\n",
    "    \"nom: Chardon prénom: Marie date_naissance: 30 lieux_naissance: \",\n",
    "    \"nom: Lhopital prénom: Louis Jean date_naissance: 67 lieux_naissance: Sn employeur: ahef\",\n",
    "    \"nom: Pyrin prénom: Marie date_naissance: 55 relation: d\",\n",
    "    \"nom: Roy prénom: Antoine date_naissance: 51 lieux_naissance: P profession: métro\"\n",
    "]\n",
    "\n",
    "\n",
    "label_mapping = {\"Femme\": 0, \"Homme\": 1}\n",
    "train_labels = [label_mapping[label] for label in [\"Femme\", \"Homme\", \"Femme\", \"Homme\"]]\n",
    "\n",
    "\n",
    "df_combined_no_nan['sex_binary_int'] = df_combined_no_nan['sex_binary'].astype(int)\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_texts, train_labels, tokenizer)\n",
    "val_texts = df_combined_no_nan['prediction'].tolist()\n",
    "val_labels = df_combined_no_nan['sex_binary_int'].tolist()\n",
    "val_dataset = MyDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,  \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,  \n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    optimizers=(torch.optim.AdamW(model.parameters(), lr=1e-5), None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 171\n",
      "  Batch size = 64\n",
      "100%|██████████| 3/3 [00:22<00:00,  7.65s/it]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 171\n",
      "  Batch size = 64\n",
      "100%|██████████| 3/3 [00:23<00:00,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4853801169590643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "result = trainer.evaluate()\n",
    "\n",
    "print(result['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "print(result['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
